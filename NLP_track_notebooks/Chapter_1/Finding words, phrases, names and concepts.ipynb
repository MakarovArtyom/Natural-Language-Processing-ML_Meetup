{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Course materials*: https://course.spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 1 headlines**:\n",
    "- data structure in SpaCy;\n",
    "- statistical models variation;\n",
    "- linguistic features prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy library allows you can establish language processing pipeline by instantiating an object. <br>\n",
    "For example, let's create **English NLP object** below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- import English language class;\n",
    "- create an object;\n",
    "- this will include language specific rules for tokenization \n",
    "\n",
    "\"\"\"\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass the text into nlp object and SpaCy will create a **doc** - document with text information in **tokenized form** (words as tokens), that be accessd using index. <br>\n",
    "So, we can **iterate over the text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hello\n",
      "1 world\n",
      "2 !\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello world!')\n",
    "for indx, token in enumerate(doc):\n",
    "    print(indx, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output above shows tokens enumerated as normal Python sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token objects** represent the tokens in document - words or punctuation.<br>\n",
    "As it shown above, we can **get a token at specific position**, indexing into the doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](doc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- for example, lets acces the second token in doc \n",
    "- apply .text attribute to display the text of token \n",
    "\"\"\"\n",
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Span object** is a basic slice of the document, that contains one or more tokens. <br>\n",
    "Closest analogy - python list slices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](span.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- slice from document with tokens \n",
    "\"\"\"\n",
    "\n",
    "span = doc[1:4]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve lexical attributes from document using indecies of tokens:\n",
    "- `.is_alpha` returns booleans that indicate whether token consists of alphabetical attributes;\n",
    "- `.is_punct` returns booleans that indicate whether token is punctuation;\n",
    "- `.like_num` returns booleans that indicate whether token is number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('It costs €10.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3, 4]\n",
      "Text: ['It', 'costs', '€', '10', '.']\n",
      "Alphabetic? [True, True, False, False, False]\n",
      "Punctuation? [False, False, False, False, True]\n",
      "Numbers [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print('Index:', [token.i for token in doc])\n",
    "print('Text:', [token.text for token in doc])\n",
    "\n",
    "print('Alphabetic?', [token.is_alpha for token in doc])\n",
    "print('Punctuation?', [token.is_punct for token in doc])\n",
    "print('Numbers', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature can be applied for lexical analysis and doc's content evaluation: how frequently numbers are used or presence of punctuation in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabetic tokens in doc: 2\n",
      "Punctuation in doc: 1\n",
      "Numbers in doc: 1\n"
     ]
    }
   ],
   "source": [
    "print('Alphabetic tokens in doc:', sum([token.is_alpha for token in doc]))\n",
    "print('Punctuation in doc:', sum([token.is_punct for token in doc]))\n",
    "print('Numbers in doc:', sum([token.like_num for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that currently more than **45 languages** are available in SpaCy library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**English example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence\n",
      "Tokens in English doc:  ['This', 'is', 'a', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp('This is a sentence')\n",
    "print(doc.text)\n",
    "print('Tokens in English doc: ', [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**German example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n",
      "Tokens in German doc:  ['Liebe', 'Grüße', '!']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.de import German \n",
    "\n",
    "nlp = German()\n",
    "\n",
    "doc = nlp('Liebe Grüße!')\n",
    "print(doc.text)\n",
    "print('Tokens in German doc: ', [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spanish example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n",
      "Tokens in Spanish doc:  ['¿', 'Cómo', 'estás', '?']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish \n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "doc = nlp('¿Cómo estás?')\n",
    "print(doc.text)\n",
    "print('Tokens in Spanish doc: ', [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create more examples docs, spans and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example of span below - slicing the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexical attribute example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SpaCy we can perform analysis of text content, for example, find specific attributes, like **percentages (%)**.<br>\n",
    "In this example, we will investigate subsequent tokens: **number + percent sign**. <br>\n",
    "Here we will iterate over tokens, using:\n",
    "- `like_num` to check whether token is a number;\n",
    "- `token.i + 1` to get token, following the token of document;\n",
    "- check whether token's attribute `text` has a percent sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical models in SpaCy can **analyze words in context**, for example:\n",
    "- whether the word is verb;\n",
    "- whether the span of text is person name. \n",
    "\n",
    "We can **predict attributes** in context:\n",
    "- part-of-speech tags;\n",
    "- syntactic dependencies;\n",
    "- named entities.\n",
    "\n",
    "**Models**, that can be used to predict in context:\n",
    "- trained on large labeled texts (so, pretrained models available); \n",
    "- possible fine-tuning: add more data, custom labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of pretrained models can be found, using `download`.<br>\n",
    "For example, `en_core_web_sm` is a small English model, that supports all core capabilities and trained on Web-text.<br>\n",
    "`spacy.load()` loads the model and returns nlp object.<br>\n",
    "Package provides **binary weights**, that enable library to make predictions. \n",
    "\n",
    "Usage: `$ python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting part of the speech tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use loaded English model to predict parts of the speach in context.<br>\n",
    "First of all we will process the text \"She ate the pizza\" (using nlp object). <br>\n",
    "After that for each token in doc we will display `.pos_` attribute, related to part of the speech tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun.<br>\n",
    "Note, in SpaCy attributes, written with `_` usually returns string. Attributes without underscore returns ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting syntactic dependancies\n",
    "\n",
    "*More about*: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can predict **how the words are related** in document. <br>\n",
    "For example, whether the word is a subject of a sequence in doc. <br>\n",
    "In this case `dep_` is used. `.head` is used to return **syntetic head** token - parent token the word is attached to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that \"She\" is a pronoun and **nominal subject** of syntetic **head** token \"ate\" (it's attached to head).<br>\n",
    "Meanwhile \"ate\" is a verb and **root** word. <br>\n",
    "Pizza will be an **direct object** of parent token \"ate\" (attached to head as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we an display **children tokens** for each token in document using `.children`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate []\n",
      "ate VERB ROOT ate [She, pizza]\n",
      "the DET det pizza []\n",
      "pizza NOUN dobj ate [the]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text,\n",
    "         [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that some words are parent tokens for others.  Determiner \"the\" and pronoun \"She\" do not own ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](dep_labels_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities are \"real world objects\", that are assigned a name (name of organisation, person or country).<br>\n",
    "We are able to use `.ents` to predict names of entitie using model.<br>\n",
    "It will return iterator of Span objects, hence we can print text and label of entity using `.label_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- process the tet as usually \n",
    "- itreration over predicted labels\n",
    "\"\"\"\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case \"Apple\" is `ORG` or organisation. U.K is `GPE` or country/city/state. <br>\n",
    "$1 billion is `MONEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly access the definition of unknown tags or labels, using `.explain` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE: Countries, cities, states\n",
      "NNP: noun, proper singular\n",
      "dobj: direct object\n"
     ]
    }
   ],
   "source": [
    "print('GPE:',spacy.explain('GPE'))\n",
    "print('NNP:',spacy.explain('NNP'))\n",
    "print('dobj:',spacy.explain('dobj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
